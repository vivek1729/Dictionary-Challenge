{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,LSTM,Conv1D,GlobalMaxPool1D,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vocab = pickle.load(open(\"train_data_filtered_vocab.pkl\", \"rb\" ))\n",
    "words,defs = pickle.load(open(\"train_data_filtered.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embs = pickle.load(open(\"../embeddings/D_cbow_pdw_8B.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_words = words[0:1000]\n",
    "training_defs = defs[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_embs = []\n",
    "for word in training_X:\n",
    "    target_embs.append(word_embs[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_embs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some research today\n",
    "We can use embedding layer directly to generate uniform embeddings for training.\n",
    "For feeding pre-trained word embeddings, we can use an embedding matrix defined in Fake news ipython nb\n",
    "\n",
    "Things to take care while inputting data:\n",
    "* Pad sequences to max length (https://keras.io/preprocessing/sequence/)\n",
    "* Use masking so that index 0 i.e. padded 0 is masked out. Good explanation on embedding layer and masking is given here (https://keras.io/layers/embeddings/). Only caveat is that we need to use vocab indices starting from 1. Should not be a problem I suppose?\n",
    "\n",
    "Second last comment on this thread gives clear examples of masking with padding for LSTM in keras (https://github.com/keras-team/keras/issues/2375)\n",
    "\n",
    "Seems like `cosine proximity` is a standard loss in Keras.\n",
    "Related links https://github.com/keras-team/keras/issues/3031, https://github.com/keras-team/keras/issues/5046\n",
    "\n",
    "Hyper params defined in the code. Cross-reference with paper\n",
    "\n",
    "\n",
    "`\n",
    "options = {\n",
    "        'dim_word': [256],\n",
    "        'dim': [512],\n",
    "        'n_layers': [1],\n",
    "        'n-words': [30000], \n",
    "        'optimizer': ['adadelta'],\n",
    "        'decay-c': [0.], \n",
    "        'use-dropout': [0],\n",
    "        'learning-rate': [0.0001],\n",
    "        'use_target_as_input': [True],\n",
    "        'reload': [False]}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create vocab\n",
    "def create_vocab(words,defs):\n",
    "    word_vocab = {}\n",
    "    #Make sure index starts with 1, since 0 is mask on\n",
    "    idx = 1\n",
    "    for word in words:\n",
    "        if word not in word_vocab:\n",
    "            word_vocab[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "    for word_def in defs:\n",
    "        for word in word_def:\n",
    "            if word not in word_vocab:\n",
    "                word_vocab[word] = idx\n",
    "                idx += 1\n",
    "    print('Unique words found in vocab ',idx)\n",
    "    return word_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Unique words found in vocab ', 2776)\n"
     ]
    }
   ],
   "source": [
    "training_vocab = create_vocab(training_words,training_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2775"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create training instances\n",
    "training_x = []\n",
    "training_y = []\n",
    "for w,d in itertools.izip(training_words,training_defs):\n",
    "    word_seq = [training_vocab[t] for t in d if t in training_vocab]\n",
    "    training_x.append(word_seq)\n",
    "    training_y.append(word_embs[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(training_x, maxlen=100, dtype='int32', padding='post', truncating='post')\n",
    "Y_train = np.array(training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 100), (1000, 500))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper params definition (from paper)\n",
    "vocab_len = len(training_vocab) + 1 #To take care of masking\n",
    "max_len = 100 #Number of timesteps\n",
    "input_emb_dim = 256 #Dimension of learned input embeddings\n",
    "output_emb_dim = 500 #Standard dim of output provided\n",
    "lstm_units = 512\n",
    "batch_size = 16\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define simple LSTM sequential model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_len, input_emb_dim, mask_zero=True, input_length=max_len))\n",
    "model.add(LSTM(lstm_units))\n",
    "model.add(Dense(output_emb_dim, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"296pt\" viewBox=\"0.00 0.00 344.89 296.00\" width=\"345pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 292)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-292 340.8867,-292 340.8867,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4821887504 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4821887504</title>\n",
       "<polygon fill=\"none\" points=\"0,-243.5 0,-287.5 336.8867,-287.5 336.8867,-243.5 0,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.7847\" y=\"-261.3\">embedding_1_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"197.5693,-243.5 197.5693,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.4038\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"197.5693,-265.5 253.2383,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.4038\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"253.2383,-243.5 253.2383,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.0625\" y=\"-272.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"253.2383,-265.5 336.8867,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.0625\" y=\"-250.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 4821888272 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4821888272</title>\n",
       "<polygon fill=\"none\" points=\"2.7139,-162.5 2.7139,-206.5 334.1729,-206.5 334.1729,-162.5 2.7139,-162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"84.7847\" y=\"-180.3\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"166.8555,-162.5 166.8555,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194.6899\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"166.8555,-184.5 222.5244,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194.6899\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"222.5244,-162.5 222.5244,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.3486\" y=\"-191.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"222.5244,-184.5 334.1729,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.3486\" y=\"-169.3\">(None, 100, 256)</text>\n",
       "</g>\n",
       "<!-- 4821887504&#45;&gt;4821888272 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4821887504-&gt;4821888272</title>\n",
       "<path d=\"M168.4434,-243.3664C168.4434,-235.1516 168.4434,-225.6579 168.4434,-216.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"171.9435,-216.6068 168.4434,-206.6068 164.9435,-216.6069 171.9435,-216.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4821888656 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>4821888656</title>\n",
       "<polygon fill=\"none\" points=\"35.3623,-81.5 35.3623,-125.5 301.5244,-125.5 301.5244,-81.5 35.3623,-81.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"84.7847\" y=\"-99.3\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"134.207,-81.5 134.207,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162.0415\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"134.207,-103.5 189.876,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162.0415\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"189.876,-81.5 189.876,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"245.7002\" y=\"-110.3\">(None, 100, 256)</text>\n",
       "<polyline fill=\"none\" points=\"189.876,-103.5 301.5244,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"245.7002\" y=\"-88.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 4821888272&#45;&gt;4821888656 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>4821888272-&gt;4821888656</title>\n",
       "<path d=\"M168.4434,-162.3664C168.4434,-154.1516 168.4434,-144.6579 168.4434,-135.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"171.9435,-135.6068 168.4434,-125.6068 164.9435,-135.6069 171.9435,-135.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4821885456 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>4821885456</title>\n",
       "<polygon fill=\"none\" points=\"46.6587,-.5 46.6587,-44.5 290.228,-44.5 290.228,-.5 46.6587,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.7847\" y=\"-18.3\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"150.9106,-.5 150.9106,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.7451\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"150.9106,-22.5 206.5796,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.7451\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"206.5796,-.5 206.5796,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.4038\" y=\"-29.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"206.5796,-22.5 290.228,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.4038\" y=\"-7.3\">(None, 500)</text>\n",
       "</g>\n",
       "<!-- 4821888656&#45;&gt;4821885456 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>4821888656-&gt;4821885456</title>\n",
       "<path d=\"M168.4434,-81.3664C168.4434,-73.1516 168.4434,-63.6579 168.4434,-54.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"171.9435,-54.6068 168.4434,-44.6068 164.9435,-54.6069 171.9435,-54.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualize model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "#plot_model(model, to_file='model_lstm.png', show_shapes=True, show_layer_names=True)\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 256)          710656    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 512)               1574912   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "=================================================================\n",
      "Total params: 2,542,068\n",
      "Trainable params: 2,542,068\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Compile model\n",
    "model.compile('adadelta',loss='cosine_proximity',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "900/900 [==============================] - 55s 61ms/step - loss: -0.5065 - acc: 0.5078 - val_loss: -0.2889 - val_acc: 0.3300\n",
      "Epoch 2/5\n",
      "900/900 [==============================] - 55s 61ms/step - loss: -0.5401 - acc: 0.6111 - val_loss: -0.2941 - val_acc: 0.3300\n",
      "Epoch 3/5\n",
      "900/900 [==============================] - 55s 62ms/step - loss: -0.5626 - acc: 0.6533 - val_loss: -0.2913 - val_acc: 0.3400\n",
      "Epoch 4/5\n",
      "900/900 [==============================] - 55s 61ms/step - loss: -0.5798 - acc: 0.7011 - val_loss: -0.2941 - val_acc: 0.3200\n",
      "Epoch 5/5\n",
      "900/900 [==============================] - 55s 62ms/step - loss: -0.5920 - acc: 0.7411 - val_loss: -0.2866 - val_acc: 0.3400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x110569d50>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start training the sequential model here\n",
    "model.fit(X_train,Y_train,batch_size=16,epochs=5,verbose=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Some utilities to compute resulting metrics as described in the paper\n",
    "To quantify the quality of a given ranking, we report\n",
    "three statistics: \n",
    "* the median rank of the correct answer (over the whole test set, lower better)\n",
    "* the proportion of training cases in which the correct answer appears in the top 10/100 in this ranking (accuracy@10/100 - higher better)\n",
    "* the variance of the rank of the correct answer across the test set (rank variance - lower better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 7 3]\n"
     ]
    }
   ],
   "source": [
    "#Calculate euclidean distance\n",
    "A = np.array([[1,1], [3,4]])\n",
    "B = np.array([[1,2]])\n",
    "a = cdist(A,B,'euclidean').flatten()\n",
    "\n",
    "#Finding top k closest points\n",
    "A = np.array([11, 7, 9, 2, 0.1, 17, 17, 1.5])\n",
    "k = 3\n",
    "idx = np.argpartition(A, 3)[:3]\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read output words and create output vocab\n",
    "output_words = [line.strip() for line in open('../Defgen_evals/output_shortlist.txt')]\n",
    "output_vocab = {k: v for v, k in enumerate(output_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prepare test data from files\n",
    "Only include words present in the output_vocab\n",
    "'''\n",
    "def create_test_data(file_name, vocab_obj):\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            test_wd = line.strip().split('\\t')\n",
    "            if test_wd[0] in vocab_obj:\n",
    "                Y_test.append(test_wd[1])\n",
    "                X_test.append(test_wd[0])\n",
    "    return X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Example usage to create test output. x and y are strings\n",
    "x,y = create_test_data('../Defgen_evals/Reverse_Dictionary/WN_seen_correct.txt',output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Returns true if label in top k closest for pred in labels\n",
    "pred - 500d emb\n",
    "label - 500d emb\n",
    "points - list of word emb of all possible output words\n",
    "k - integer (10/100)\n",
    "'''\n",
    "def find_top_k_dist (points, pred, label, k):\n",
    "    label_idx = output_vocab[labael]\n",
    "    dist_list = cdist(points, np.array([pred]), 'euclidean').flatten() #Convert 2d into 1d array\n",
    "    result_indices = np.argpartition(dist_list, k)[:k]\n",
    "    if label_idx in result_indices:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Run function for some test data to confirm correctness\n",
    "* Train on entire data set and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
