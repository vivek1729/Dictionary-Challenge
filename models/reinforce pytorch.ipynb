{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import math\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words,defs = pickle.load(open(\"train_data_filtered.pkl\", \"rb\" ),encoding='latin1')\n",
    "word_embs = pickle.load(open(\"../embeddings/D_cbow_pdw_8B.pkl\", \"rb\" ),encoding='latin1')\n",
    "#Based on https://stackoverflow.com/questions/32957708/python-pickle-error-unicodedecodeerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Selecting subset of data\n",
    "training_words = words[:1000]\n",
    "training_defs = defs[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create vocab\n",
    "def create_vocab(words,defs):\n",
    "    x_vocab = {}\n",
    "    y_vocab = {}\n",
    "    x_idx = 0\n",
    "    y_idx = 0\n",
    "    for word in words:\n",
    "        if word not in y_vocab:\n",
    "            y_vocab[word] = y_idx\n",
    "            y_idx += 1\n",
    "\n",
    "    for word_def in defs:\n",
    "        for word in word_def:\n",
    "            if word not in x_vocab:\n",
    "                x_vocab[word] = x_idx\n",
    "                x_idx += 1\n",
    "    x_vocab['<pad>'] = x_idx #Pad token is last index useed for masking\n",
    "    y_vocab['<pad>'] = y_idx #Pad token is last index useed for masking\n",
    "    print('Unique words found in x_vocab ',len(x_vocab))\n",
    "    print('Unique words found in y_vocab ',len(y_vocab))\n",
    "    return x_vocab,y_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words found in x_vocab  2739\n",
      "Unique words found in y_vocab  75\n"
     ]
    }
   ],
   "source": [
    "X_vocab,Y_vocab = create_vocab(training_words,training_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper params definition (from paper)\n",
    "vocab_len = len(X_vocab)\n",
    "max_len = 16 #Number of timesteps\n",
    "input_emb_dim = 500 #Dimension of learned input embeddings\n",
    "output_emb_dim = 500 #Standard dim of output provided\n",
    "lstm_units = 512\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "train_val_split = 0.85 #85% train, 15% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_x = [[X_vocab[word] for word in word_def] for word_def in training_defs]\n",
    "training_y = [Y_vocab[word] for word in training_words]\n",
    "#Pad sequences, prepare training data\n",
    "X = np.ones((len(training_x), max_len)) * 0\n",
    "for i in range(len(training_x)):\n",
    "    seq = training_x[i]\n",
    "    seq_len = min(len(seq),max_len)\n",
    "    X[i,0:seq_len] = seq[0:seq_len]\n",
    "Y = np.array(training_y)\n",
    "data_set = []\n",
    "for i in range(len(training_x)):\n",
    "    data_set.append((X[i],Y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n"
     ]
    }
   ],
   "source": [
    "#Split dataset into training and validation set\n",
    "np.random.shuffle(data_set)\n",
    "val_idx = math.floor(train_val_split*len(data_set))\n",
    "print(val_idx)\n",
    "train_set = data_set[:val_idx]\n",
    "val_set = data_set[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchIterator:\n",
    "\n",
    "    \"\"\"Iterator that returns batches of fixed sizes for specified epochs\"\"\"\n",
    "\n",
    "    def __init__(self, data_set, batch_size, epochs):\n",
    "        self.data_set = data_set\n",
    "        self.start_idx = 0\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.len_data_set = len(self.data_set)\n",
    "        self.current_epoch = 1\n",
    "        self.current_batch = 1       \n",
    "        self.batches_per_epoch = math.ceil(self.len_data_set/self.batch_size)\n",
    "        print(self.len_data_set)\n",
    "        print(self.batches_per_epoch)\n",
    "        np.random.shuffle(self.data_set)\n",
    "    def has_ended(self):\n",
    "        return self.current_epoch > self.epochs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        #print('Current start index '+str(self.start_idx))\n",
    "        #print('Current epoch, current batch '+str(self.current_epoch)+' , '+str(self.current_batch))\n",
    "        batch = []\n",
    "        i = self.start_idx\n",
    "        end = min(self.len_data_set-1,i+self.batch_size-1)\n",
    "        #print(end)\n",
    "        while(i <= end):\n",
    "            batch.append(self.data_set[i])\n",
    "            i += 1\n",
    "        #If can't find enough data points till the end of list, start from zero again\n",
    "        if(len(batch) < self.batch_size):\n",
    "            #print('Over shot the length of the list')\n",
    "            i = 0\n",
    "            while(len(batch) != self.batch_size):\n",
    "                batch.append(self.data_set[i]) \n",
    "                i += 1\n",
    "        self.start_idx = i\n",
    "        \n",
    "        #Increment epoch, reset variables, reshuffle the dataset\n",
    "        if(self.current_batch == self.batches_per_epoch):\n",
    "            self.current_epoch += 1            \n",
    "            self.current_batch = 1\n",
    "            #Shuffle the list, reset start idx\n",
    "            np.random.shuffle(self.data_set)\n",
    "            self.start_idx = 0\n",
    "        else:\n",
    "            self.current_batch += 1\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding not found\n",
      "Embedding matrix created!\n"
     ]
    }
   ],
   "source": [
    "#Create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_len, output_emb_dim))\n",
    "for word, i in X_vocab.items():\n",
    "    embedding_vector = word_embs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print('Embedding not found')\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(output_emb_dim, ))\n",
    "print(\"Embedding matrix created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyEstimator(nn.Module):\n",
    "\n",
    "    def __init__(self, weight_matrix, embedding_dim, hidden_dim, vocab_size, tagset_size, seq_len, batch_size):\n",
    "        super(PolicyEstimator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.tagset_size = tagset_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        #self.word_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        #Load pretrained word embeddings\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(weight_matrix)\n",
    "        #Trainable false\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def predict(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        X = lstm_out.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "        tag_space = self.hidden2tag(X)\n",
    "        tag_scores = F.softmax(tag_space, dim=1) #Return softmax here!\n",
    "        Y_hat = tag_scores.view(self.batch_size, self.seq_len, self.tagset_size)\n",
    "        return tag_space, Y_hat\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        X = lstm_out.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "        tag_space = self.hidden2tag(X)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1) #Return log softmax for forward function!\n",
    "        Y_hat = tag_scores.view(self.batch_size, self.seq_len, self.tagset_size)\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# points should have embeddings of all output words in a list\n",
    "#Normalize output word embeddings to unit vectors\n",
    "def normalize_vocab(voc):\n",
    "    points = [None]*(len(voc))\n",
    "    for w, idx in voc.items():\n",
    "        points[idx] = embedding_matrix[idx]/np.linalg.norm(embedding_matrix[idx])\n",
    "    points = np.array(points)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = normalize_vocab(Y_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pred_rank (pred_idx, label_idx, points):\n",
    "    dist_list = cdist(points, np.array([points[pred_idx]]), 'cosine').flatten() #Convert 2d into 1d array\n",
    "    #result_indices = np.argpartition(dist_list, k)[:k]\n",
    "    #if label_idx in result_indices:\n",
    "    #    return True\n",
    "    #return False\n",
    "    result_indices = list(np.argsort(dist_list))\n",
    "    return result_indices.index(label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reward_for_action(action,target):\n",
    "    #Using inverse rank as reward\n",
    "    rank = get_pred_rank(action, target, points) + 1\n",
    "    return 1/rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(policy_estimator, data_iterator):\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(policy_estimator.parameters(),lr=0.01)\n",
    "    total_rewards = []\n",
    "    counter = 0\n",
    "    loss_freq = 10 #Print loss every 10 batches\n",
    "    val_freq = 100 #Print validation accuracy every 100 batches\n",
    "    #Save model based on some acc threshold\n",
    "    losses=[]\n",
    "    while(not data_iterator.has_ended()):\n",
    "        policy_estimator.hidden = policy_estimator.init_hidden()\n",
    "        action_space = np.arange(policy_estimator.tagset_size)\n",
    "        \n",
    "        #Get next batch\n",
    "        batch_data = next(data_iterator)\n",
    "        counter += 1\n",
    "        batch_rewards = []\n",
    "        batch_actions = []\n",
    "        batch_states = [sequence[0] for sequence in batch_data]\n",
    "        state_tensor = torch.tensor(batch_states, dtype=torch.long)\n",
    "        \n",
    "        _, batch_out = policy_estimator.predict(state_tensor)\n",
    "        \n",
    "        batch_out = batch_out.detach().numpy()\n",
    "        for i in range(len(batch_data)):\n",
    "            #Iterate over each LSTM time step\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            target = batch_data[i][1] #Get label of the current sequence\n",
    "            for j in range(batch_out[i].shape[0]):\n",
    "                action_probs = batch_out[i][j]\n",
    "                action = np.random.choice(action_space, p=action_probs)\n",
    "                actions.append([action]) #To have 1 dimension at the end for indexing\n",
    "                reward = get_reward_for_action(action,target)\n",
    "                rewards.append(reward)                   \n",
    "            batch_actions.append(actions)\n",
    "            batch_rewards.append(np.mean(rewards)) #Append average reward per time step to batch_rewards                   \n",
    "        total_rewards.append(sum(batch_rewards)) #Total of rewards for a batch\n",
    "        reward_tensor = torch.FloatTensor(batch_rewards)\n",
    "        \n",
    "        action_tensor = torch.LongTensor(batch_actions)\n",
    "\n",
    "        #Gradient update\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate loss\n",
    "        logprob = policy_estimator(state_tensor)\n",
    "        \n",
    "        '''\n",
    "        Dimensionality check\n",
    "        logprob = batch_size * max_len * tagset_size(output_vocab_dim)\n",
    "        reward_tensor = batch_size * max_len\n",
    "        action_tensor = batch_size * max_len * 1 (3d tensor so that I can do the gather operation)\n",
    "        '''\n",
    "        \n",
    "        selected_logprobs = torch.gather(logprob,2,action_tensor)\n",
    "        #Reshape so that dot product can be computed\n",
    "        selected_logprobs = selected_logprobs.view(selected_logprobs.shape[0],selected_logprobs.shape[1])\n",
    "        selected_logprobs = reward_tensor * selected_logprobs\n",
    "        \n",
    "        loss = -selected_logprobs.mean(1).sum() #Take mean of timestep for loss per seq, sum for all seq in batch\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        if(counter % loss_freq == 0):\n",
    "            print(\"\\rEpoch : {}, Batch count: {} Current Loss: {:.4f} Rewards of last 10 batches: {:.2f}\".format(\n",
    "                data_iterator.current_epoch, counter, loss, np.mean(total_rewards[-10:]) ))\n",
    "\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradients\n",
    "        optimizer.step()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize weight matrix tensor from embedding matrix\n",
    "weight_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataIterator\n",
    "batchIterator = BatchIterator(train_set, batch_size, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = PolicyEstimator(weight_matrix, input_emb_dim, lstm_units, len(X_vocab), len(Y_vocab), max_len, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Batch count: 10 Current Loss: 1.4277 Rewards of last 10 batches: 1.21\n",
      "Epoch : 1, Batch count: 20 Current Loss: 0.9788 Rewards of last 10 batches: 1.02\n",
      "Epoch : 1, Batch count: 30 Current Loss: 0.2887 Rewards of last 10 batches: 1.04\n",
      "Epoch : 1, Batch count: 40 Current Loss: 0.5445 Rewards of last 10 batches: 1.52\n",
      "Epoch : 1, Batch count: 50 Current Loss: 0.0453 Rewards of last 10 batches: 1.14\n",
      "Epoch : 2, Batch count: 60 Current Loss: 0.0543 Rewards of last 10 batches: 1.09\n",
      "Epoch : 2, Batch count: 70 Current Loss: 0.0514 Rewards of last 10 batches: 1.29\n",
      "Epoch : 2, Batch count: 80 Current Loss: 0.0176 Rewards of last 10 batches: 1.04\n",
      "Epoch : 2, Batch count: 90 Current Loss: 0.0436 Rewards of last 10 batches: 0.89\n",
      "Epoch : 2, Batch count: 100 Current Loss: 0.2733 Rewards of last 10 batches: 0.75\n",
      "Epoch : 3, Batch count: 110 Current Loss: 0.2052 Rewards of last 10 batches: 0.98\n",
      "Epoch : 3, Batch count: 120 Current Loss: 0.0258 Rewards of last 10 batches: 0.96\n",
      "Epoch : 3, Batch count: 130 Current Loss: 0.0812 Rewards of last 10 batches: 0.66\n",
      "Epoch : 3, Batch count: 140 Current Loss: 0.1093 Rewards of last 10 batches: 0.80\n",
      "Epoch : 3, Batch count: 150 Current Loss: 0.0351 Rewards of last 10 batches: 0.70\n",
      "Epoch : 3, Batch count: 160 Current Loss: 0.0208 Rewards of last 10 batches: 1.02\n",
      "Epoch : 4, Batch count: 170 Current Loss: 0.0535 Rewards of last 10 batches: 0.88\n",
      "Epoch : 4, Batch count: 180 Current Loss: 0.0218 Rewards of last 10 batches: 0.80\n",
      "Epoch : 4, Batch count: 190 Current Loss: 0.0458 Rewards of last 10 batches: 0.74\n",
      "Epoch : 4, Batch count: 200 Current Loss: 0.0159 Rewards of last 10 batches: 0.90\n",
      "Epoch : 4, Batch count: 210 Current Loss: 0.0131 Rewards of last 10 batches: 0.80\n"
     ]
    }
   ],
   "source": [
    "output_losses = train(model, batchIterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x114935a20>]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot output losses\n",
    "pl.plot(output_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
