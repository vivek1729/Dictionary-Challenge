{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import math\n",
    "import pylab as pl\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CTX_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "print('Using device '+str(CTX_DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words,defs = pickle.load(open(\"train_data_filtered.pkl\", \"rb\" ),encoding='latin1')\n",
    "word_embs = pickle.load(open(\"../embeddings/D_cbow_pdw_8B.pkl\", \"rb\" ),encoding='latin1')\n",
    "#Based on https://stackoverflow.com/questions/32957708/python-pickle-error-unicodedecodeerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Selecting subset of data\n",
    "training_words = words\n",
    "training_defs = defs\n",
    "data = []\n",
    "for i in range(len(training_words)):\n",
    "    data.append((training_words[i],training_defs[i]))\n",
    "np.random.shuffle(data)\n",
    "data = data[0:1000]\n",
    "training_words = []\n",
    "training_defs = []\n",
    "for i in range(len(data)):\n",
    "    training_words.append(data[i][0])\n",
    "    training_defs.append(data[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create vocab\n",
    "def create_vocab(words,defs):\n",
    "    x_vocab = {}\n",
    "    y_vocab = {}\n",
    "    x_idx = 0\n",
    "    y_idx = 0\n",
    "    for word in words:\n",
    "        if word not in y_vocab:\n",
    "            y_vocab[word] = y_idx\n",
    "            y_idx += 1\n",
    "\n",
    "    for word_def in defs:\n",
    "        for word in word_def:\n",
    "            if word not in x_vocab:\n",
    "                x_vocab[word] = x_idx\n",
    "                x_idx += 1\n",
    "    x_vocab['<pad>'] = x_idx #Pad token is last index useed for masking\n",
    "    y_vocab['<pad>'] = y_idx #Pad token is last index useed for masking\n",
    "    print('Unique words found in x_vocab ',len(x_vocab))\n",
    "    print('Unique words found in y_vocab ',len(y_vocab))\n",
    "    return x_vocab,y_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words found in x_vocab  3891\n",
      "Unique words found in y_vocab  974\n"
     ]
    }
   ],
   "source": [
    "X_vocab,Y_vocab = create_vocab(training_words,training_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper params definition (from paper)\n",
    "vocab_len = len(X_vocab)\n",
    "max_len = 24 #Number of timesteps\n",
    "input_emb_dim = 500 #Dimension of learned input embeddings\n",
    "output_emb_dim = 500 #Standard dim of output provided\n",
    "lstm_units = 512\n",
    "batch_size = 4\n",
    "epochs = 2\n",
    "train_val_split = 0.90 #90% train, 10% validation\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99 #Discount factor\n",
    "MODEL_PATH = \"rlModelState.tar\"\n",
    "CSV_LOG_FILE = \"training_log.csv\"\n",
    "USE_BASELINE = True\n",
    "USE_DISCOUNT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_x = [[X_vocab[word] for word in word_def] for word_def in training_defs]\n",
    "training_y = [Y_vocab[word] for word in training_words]\n",
    "#Pad sequences, prepare training data\n",
    "X = np.ones((len(training_x), max_len)) * 0\n",
    "for i in range(len(training_x)):\n",
    "    seq = training_x[i]\n",
    "    seq_len = min(len(seq),max_len)\n",
    "    X[i,0:seq_len] = seq[0:seq_len]\n",
    "Y = np.array(training_y)\n",
    "data_set = []\n",
    "for i in range(len(training_x)):\n",
    "    data_set.append((X[i],Y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "Training and validation sets created\n"
     ]
    }
   ],
   "source": [
    "#Split dataset into training and validation set\n",
    "np.random.shuffle(data_set)\n",
    "val_idx = math.floor(train_val_split*len(data_set))\n",
    "print(val_idx)\n",
    "train_set = data_set[:val_idx]\n",
    "val_set = data_set[val_idx:]\n",
    "print('Training and validation sets created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchIterator:\n",
    "\n",
    "    \"\"\"Iterator that returns batches of fixed sizes for specified epochs\"\"\"\n",
    "\n",
    "    def __init__(self, data_set, batch_size, epochs):\n",
    "        self.data_set = data_set\n",
    "        self.start_idx = 0\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.len_data_set = len(self.data_set)\n",
    "        self.current_epoch = 1\n",
    "        self.current_batch = 1       \n",
    "        self.batches_per_epoch = math.ceil(self.len_data_set/self.batch_size)\n",
    "        print(self.len_data_set)\n",
    "        print(self.batches_per_epoch)\n",
    "        np.random.shuffle(self.data_set)\n",
    "    def has_ended(self):\n",
    "        return self.current_epoch > self.epochs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        #print('Current start index '+str(self.start_idx))\n",
    "        #print('Current epoch, current batch '+str(self.current_epoch)+' , '+str(self.current_batch))\n",
    "        batch = []\n",
    "        i = self.start_idx\n",
    "        end = min(self.len_data_set-1,i+self.batch_size-1)\n",
    "        #print(end)\n",
    "        while(i <= end):\n",
    "            batch.append(self.data_set[i])\n",
    "            i += 1\n",
    "        #If can't find enough data points till the end of list, start from zero again\n",
    "        if(len(batch) < self.batch_size):\n",
    "            #print('Over shot the length of the list')\n",
    "            i = 0\n",
    "            while(len(batch) != self.batch_size):\n",
    "                batch.append(self.data_set[i]) \n",
    "                i += 1\n",
    "        self.start_idx = i\n",
    "        \n",
    "        #Increment epoch, reset variables, reshuffle the dataset\n",
    "        if(self.current_batch == self.batches_per_epoch):\n",
    "            self.current_epoch += 1            \n",
    "            self.current_batch = 1\n",
    "            #Shuffle the list, reset start idx\n",
    "            np.random.shuffle(self.data_set)\n",
    "            self.start_idx = 0\n",
    "        else:\n",
    "            self.current_batch += 1\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding not found\n",
      "Embedding matrix created!\n"
     ]
    }
   ],
   "source": [
    "#Create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_len, output_emb_dim))\n",
    "for word, i in X_vocab.items():\n",
    "    embedding_vector = word_embs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print('Embedding not found')\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(output_emb_dim, ))\n",
    "print(\"Embedding matrix created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyEstimator(nn.Module):\n",
    "\n",
    "    def __init__(self, weight_matrix, embedding_dim, hidden_dim, vocab_size, tagset_size, seq_len, batch_size):\n",
    "        super(PolicyEstimator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.tagset_size = tagset_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        #self.word_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        #Load pretrained word embeddings\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(weight_matrix)\n",
    "        #Trainable false\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        if(USE_GPU):\n",
    "            return (torch.zeros(1, self.batch_size, self.hidden_dim, device=CTX_DEVICE),\n",
    "                    torch.zeros(1, self.batch_size, self.hidden_dim, device=CTX_DEVICE))\n",
    "        else:\n",
    "            return (torch.zeros(1, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def predict(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        X = lstm_out.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "        tag_space = self.hidden2tag(X)\n",
    "        tag_scores = F.softmax(tag_space, dim=1) #Return softmax here!\n",
    "        Y_hat = tag_scores.view(self.batch_size, self.seq_len, self.tagset_size)\n",
    "        return Y_hat\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        X = lstm_out.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "        tag_space = self.hidden2tag(X)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1) #Return log softmax for forward function!\n",
    "        Y_hat = tag_scores.view(self.batch_size, self.seq_len, self.tagset_size)\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# points should have embeddings of all output words in a list\n",
    "#Normalize output word embeddings to unit vectors\n",
    "def normalize_vocab(voc):\n",
    "    points = [None]*(len(voc))\n",
    "    for w, idx in voc.items():\n",
    "        points[idx] = embedding_matrix[idx]/np.linalg.norm(embedding_matrix[idx])\n",
    "    points = np.array(points)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = normalize_vocab(Y_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pred_rank_old (pred_idx, label_idx, points):\n",
    "    dist_list = cdist(points, np.array([points[pred_idx]]), 'cosine').flatten() #Convert 2d into 1d array\n",
    "    #result_indices = np.argpartition(dist_list, k)[:k]\n",
    "    #if label_idx in result_indices:\n",
    "    #    return True\n",
    "    #return False\n",
    "    result_indices = list(np.argsort(dist_list))\n",
    "    return result_indices.index(label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pred_rank (pred_idx, label_idx, points):\n",
    "    rank = 200\n",
    "    c = np.argsort(points)\n",
    "    c = list(c[::-1])\n",
    "    label_idx = c.index(label_idx)\n",
    "    pred_idx = c.index(pred_idx)\n",
    "    rank = abs(label_idx - pred_idx)\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reward_for_action(action, target, action_probs):\n",
    "    #Use a sparse ranking function, softmax returns probs\n",
    "    c = np.argsort(action_probs)\n",
    "    c = list(c[::-1])\n",
    "    label_idx = c.index(target)\n",
    "    pred_idx = c.index(action)\n",
    "    r = 1/(abs(label_idx - pred_idx) + 1)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    '''\n",
    "    (val_set) => (acc, top10, top100)\n",
    "    form chunks of batch size\n",
    "    get predictions for each time step from the LSTM\n",
    "    consider output of last time step as pred of LSTM\n",
    "    call rank function get rank, calculate acc\n",
    "    '''\n",
    "    val_results = []\n",
    "    count_10 = 0\n",
    "    count_100 = 0\n",
    "    i = 0\n",
    "    while i < len(val_set) and len(val_set) > len(val_results):       \n",
    "        val_batch = []\n",
    "        val_batch = val_set[i:i+batch_size]\n",
    "        term_idx = batch_size\n",
    "        if(len(val_batch) < batch_size):\n",
    "            term_idx = len(val_batch)\n",
    "            val_batch += val_set[0:batch_size-len(val_batch)]\n",
    "        val_states = [sequence[0] for sequence in val_batch]\n",
    "        val_tensor = torch.tensor(val_states, dtype=torch.long) \n",
    "        if(USE_GPU):\n",
    "            val_tensor = val_tensor.cuda()   \n",
    "        val_out = model.predict(val_tensor)\n",
    "        val_out = val_out.cpu().detach().numpy()\n",
    "        i += batch_size\n",
    "        #print(val_out.shape)\n",
    "        for j in range(batch_size):\n",
    "            if(j < term_idx):\n",
    "                val_target = val_batch[j][1]\n",
    "                #print(val_out[i])\n",
    "                #print('=====')\n",
    "                #print(val_out[i][batch_size-1])\n",
    "                val_pred = np.argmax(val_out[j][max_len-1])\n",
    "                val_actions = val_out[j][max_len-1]\n",
    "                #print(val_target,val_pred)\n",
    "                rank = get_pred_rank(val_pred, val_target, val_actions)\n",
    "                if(rank < 10):\n",
    "                    count_10 += 1\n",
    "                if(rank < 100):\n",
    "                    count_100 += 1\n",
    "                #print(rank)\n",
    "                val_results.append(rank)\n",
    "    #print('Iter done')\n",
    "    #print(len(val_results),len(val_set))\n",
    "    return {'t_10':count_10,'t_100':count_100,'l':len(val_results)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_accuracy(batch_data, batch_out):\n",
    "    count_10 = 0\n",
    "    count_100 = 0\n",
    "    for j in range(batch_size):\n",
    "        train_target = batch_data[j][1]\n",
    "        train_pred = np.argmax(batch_out[j][max_len-1])\n",
    "        train_actions = batch_out[j][max_len-1]\n",
    "        #print(val_target,val_pred)\n",
    "        rank = get_pred_rank(train_pred, train_target, train_actions)\n",
    "        if(rank < 10):\n",
    "            count_10 += 1\n",
    "        if(rank < 100):\n",
    "            count_100 += 1\n",
    "    return {'t_10':count_10,'t_100':count_100,'l':batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(policy_estimator, data_iterator):\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(policy_estimator.parameters(),lr=learning_rate)\n",
    "    total_rewards = []\n",
    "    counter = 0\n",
    "    loss_freq = 2 #Print loss every 10 batches   \n",
    "    losses=[]\n",
    "    last_epoch = 1\n",
    "    min_epoch_loss = 1000\n",
    "    epoch_loss = 0\n",
    "    epoch_train_10 = 0\n",
    "    epoch_train_100 = 0\n",
    "    epoch_inst_count = 0\n",
    "    epoch_batch_count = 0\n",
    "    reward_history = np.zeros((2*batch_size,max_len))\n",
    "    \n",
    "    while(not data_iterator.has_ended()):\n",
    "        policy_estimator.hidden = policy_estimator.init_hidden()\n",
    "        action_space = np.arange(policy_estimator.tagset_size)\n",
    "        \n",
    "        #Get next batch\n",
    "        batch_data = next(data_iterator)\n",
    "        counter += 1\n",
    "        epoch_batch_count += 1\n",
    "        batch_rewards = []\n",
    "        batch_actions = []\n",
    "        batch_states = [sequence[0] for sequence in batch_data]\n",
    "        state_tensor = torch.tensor(batch_states, dtype=torch.long)\n",
    "        if(USE_GPU):\n",
    "            state_tensor = state_tensor.cuda()\n",
    "        \n",
    "        batch_out = policy_estimator.predict(state_tensor)\n",
    "        batch_out = batch_out.cpu().detach().numpy()\n",
    "        \n",
    "        #Compute training accuracy\n",
    "        train_acc = get_training_accuracy(batch_data, batch_out)\n",
    "        epoch_train_10 += train_acc['t_10']\n",
    "        epoch_train_100 += train_acc['t_100']\n",
    "        epoch_inst_count += batch_size\n",
    "        \n",
    "        rew_idx = batch_size\n",
    "        for i in range(len(batch_data)):\n",
    "            #Iterate over each LSTM time step\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            target = batch_data[i][1] #Get label of the current sequence\n",
    "            for j in range(batch_out[i].shape[0]):\n",
    "                action_probs = batch_out[i][j]\n",
    "                action = np.random.choice(action_space, p=action_probs)\n",
    "                actions.append([action]) #To have 1 dimension at the end for indexing\n",
    "                reward = get_reward_for_action(action, target, action_probs)\n",
    "                if(USE_DISCOUNT):\n",
    "                    reward = math.pow(gamma,max_len-1-j)*reward\n",
    "                reward_history[rew_idx][j] = reward\n",
    "                baseline = 0\n",
    "                if(USE_BASELINE):\n",
    "                    #Take moving average of the last 1 batch\n",
    "                    baseline = reward_history[rew_idx-batch_size:rew_idx,j].mean()\n",
    "                rewards.append(reward-baseline)\n",
    "            rew_idx += 1\n",
    "            batch_actions.append(actions)\n",
    "            batch_rewards.append(rewards) #Append average reward per time step to batch_rewards\n",
    "        \n",
    "        #Adjust rewards history array\n",
    "        rew_idx = batch_size\n",
    "        reward_history[0:batch_size] = reward_history[batch_size:2*batch_size]\n",
    "        reward_history[batch_size:2*batch_size] = 0\n",
    "        \n",
    "        #total_rewards.append(sum(batch_rewards)) #Total of rewards for a batch\n",
    "        reward_tensor = torch.FloatTensor(batch_rewards)\n",
    "        action_tensor = torch.LongTensor(batch_actions)\n",
    "        if(USE_GPU):\n",
    "            reward_tensor = reward_tensor.cuda()\n",
    "            action_tensor = action_tensor.cuda()\n",
    "\n",
    "        #Gradient update\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate loss\n",
    "        logprob = policy_estimator(state_tensor)\n",
    "        \n",
    "        '''\n",
    "        Dimensionality check\n",
    "        logprob = batch_size * max_len * tagset_size(output_vocab_dim)\n",
    "        reward_tensor = batch_size * max_len\n",
    "        action_tensor = batch_size * max_len * 1 (3d tensor so that I can do the gather operation)\n",
    "        '''\n",
    "        \n",
    "        selected_logprobs = torch.gather(logprob,2,action_tensor)\n",
    "        #Reshape so that dot product can be computed\n",
    "        selected_logprobs = selected_logprobs.view(selected_logprobs.shape[0],selected_logprobs.shape[1])\n",
    "        selected_logprobs = reward_tensor * selected_logprobs\n",
    "        \n",
    "        #loss = -selected_logprobs.mean(1).sum() #Take mean of timestep for loss per seq, sum for all seq in batch\n",
    "        loss = -selected_logprobs.sum()\n",
    "        #Accumulate epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if(counter % loss_freq == 0):\n",
    "            val_results = validate()\n",
    "            rl_loss = round(loss.item(),4)\n",
    "            print(\"\\rEpoch : {}, sn: {} Loss: {:.4f} 10 ta: {:.2f} 100 ta: {:.2f} 10 va: {:.2f} 100 va: {:.2f}\".format(\n",
    "                data_iterator.current_epoch, counter, rl_loss,\n",
    "                epoch_train_10/epoch_inst_count,\n",
    "                epoch_train_100/epoch_inst_count,\n",
    "                val_results['t_10']/val_results['l'],\n",
    "                val_results['t_100']/val_results['l']))\n",
    "            \n",
    "            #Write these results to the csv log file            \n",
    "            log_writer.writerow([data_iterator.current_epoch, counter, rl_loss, \n",
    "                                 val_results['t_10']/val_results['l'],\n",
    "                                 val_results['t_100']/val_results['l']])\n",
    "        \n",
    "        if(last_epoch != data_iterator.current_epoch):\n",
    "            epoch_loss = epoch_loss/epoch_batch_count\n",
    "            #losses.append(epoch_loss)\n",
    "            \n",
    "            #Reset epoch counts now\n",
    "            epoch_loss = 0\n",
    "            epoch_train_10 = 0\n",
    "            epoch_train_100 = 0\n",
    "            epoch_inst_count = 0\n",
    "            epoch_batch_count = 0\n",
    "            \n",
    "            #Save the model if loss has decreased\n",
    "            if(min_epoch_loss > loss.item()):\n",
    "                min_epoch_loss = loss.item()\n",
    "                print(\"Loss improved from {:.4f} to {:.4f}\".format(\n",
    "                    min_epoch_loss, loss.item()\n",
    "                ))\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, MODEL_PATH)\n",
    "                print(\"Last checkpoint at \"+str(datetime.datetime.now()))\n",
    "            last_epoch = data_iterator.current_epoch\n",
    "\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradients\n",
    "        optimizer.step()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize weight matrix tensor from embedding matrix\n",
    "if(USE_GPU):\n",
    "    weight_matrix = torch.FloatTensor(embedding_matrix, device=CTX_DEVICE)\n",
    "else:\n",
    "    weight_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataIterator\n",
    "batchIterator = BatchIterator(train_set, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = PolicyEstimator(weight_matrix, input_emb_dim, lstm_units, len(X_vocab), len(Y_vocab), max_len, batch_size)\n",
    "if(USE_GPU):\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting training...')\n",
    "csv_log_file = open(CSV_LOG_FILE, mode='w')\n",
    "log_writer = csv.writer(csv_log_file)\n",
    "output_losses = train(model, batchIterator)\n",
    "csv_log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11bacff28>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD8CAYAAACINTRsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXecFtXVx39nC0vvCyJtaYoIUlwRFFFREEQF0aDGgi0YY4zGvFGwIEaT2CLRxBobGltiVFQUpNkFpPfee+/L1vv+8cw8O8880/s8e76fD+zzzHNn5sydO+eeOffcc0kIAYZhGCYzyApbAIZhGMY7WKkzDMNkEKzUGYZhMghW6gzDMBkEK3WGYZgMgpU6wzBMBsFKnWEYJoNgpc4wDJNBsFJnGIbJIHKCPFnjxo1FQUFBkKdkGIaJPXPnzt0jhMi3UjZQpV5QUIA5c+YEeUqGYZjYQ0QbrZZl9wvDMEwGwUqdYRgmg2ClzjAMk0GwUmcYhskgWKkzDMNkEKzUGYZhMghW6gzDMBkEK3WGYULnSHEZPpm/NWwxMoJAJx8xDMNo8cDHizFhwTa0za+F01rUD1ucWMOWOsMwobP94HEAwLGS8pAliT+s1BmGYTIIU6VORCcT0QLFv0NEdDcRNSSiKUS0WvrbIAiBGYZhGH1MlboQYqUQopsQohuA0wEcA/AxgFEApgkhOgCYJn1nGIZhQsSu++UCAGuFEBsBDAEwXto+HsBQLwVjGIZh7GNXqV8N4D3pc1MhxHbp8w4ATT2TimGYqoUIW4DMwbJSJ6JqAC4D8F/1b0IIAZ3bQkQjiWgOEc3ZvXu3Y0EZhmEYc+xY6oMAzBNC7JS+7ySiZgAg/d2ltZMQ4hUhRKEQojA/39LCHQzDVDUobAEyBztK/RpUul4A4FMAI6TPIwBM8EoohmEYxhmWlDoR1QLQH8BHis2PA+hPRKsBXCh9ZxiGYULEUpoAIcRRAI1U2/YiEQ3DMAzDRASeUcowDJNBsFJnGIbJIFipMwzDZBCs1BmGYTIIVuoMw4QPzyj1DFbqDMMwGQQrdYZhwodnlHoGK3WGYcKH3S+ewUqdYRgmg2ClzjBM+LD7xTNYqTMMw2QQrNQZhgkf9ql7Bit1hmGYDIKVOsMw4cM+dc9gpc4wTPiw+8UzWKkzDMNkEKzUGYYJH3a/eAYrdYZhwofdL57BSp1hmMjABrt7WKkzDBMZ2GB3Dyt1hmGYDIKVOsMwkYHdL+5hpc4wTGRg94t7WKkzDBM+bKJ7Bit1hmHCx0cTfe7Gfdh16Lh/J4gYlpQ6EdUnog+JaAURLSei3kTUkIimENFq6W8Dv4VlGIaxyxUv/oT+474NW4zAsGqpPwtgkhCiI4CuAJYDGAVgmhCiA4Bp0neGYRj7+Ox+OVhU6u8JIoSpUieiegD6AngNAIQQJUKIAwCGABgvFRsPYKhfQjIMk+HEdIT0eGk59h8tCVuMFKxY6m0A7AbwBhHNJ6JXiagWgKZCiO1SmR0AmmrtTEQjiWgOEc3ZvXu3N1IzTIQoKatAaXlF2GJkBHEbLx36/A/o/uiUsMVIwYpSzwHQA8CLQojuAI5C5WoRQgjo9LVCiFeEEIVCiML8/Hy38jJM5OgydjIKH5sathgZQdwM9hU7DoctQhpWlPoWAFuEELOk7x8ioeR3ElEzAJD+7vJHRIaJNsVlFVXKZ8tEG1OlLoTYAWAzEZ0sbboAwDIAnwIYIW0bAWCCLxIyDFNliJv7JYrkWCx3J4B3iKgagHUAbkKiQ/gPEd0CYCOA4f6IyDBMpnK0uAw7FTHkcXO/RBFLSl0IsQBAocZPF3grDsMwVYmb3vwZs9fvQ882DcMWJWPgGaUMw4TG7PX7Eh98MtGHv/yTPweOMKzUGYbRpGDURDw9eWWg5/Tap57sNKoQrNQZhtHlnzPWBHo+9qm7h5U6wzBMBsFKnWGYyMAhje5hpc4wTGRg94t7WKkzDMNkEKzUGaaK8+LXazFpyXbzgj4iJBud3S/usTqjlGGYDOWJSSsAABseHxyaDIL9Lp7BljrDMEwGwUqdYZjQYUPdO1ipM0yGMm/TfkxassPRvsKFP6SsvALPz1iDYyVltvc9UFSKDXuOOj43w0qdYTKWYS/8iF//e27g5/1s0TY8NXklnp68yva+t709F+c9/bX3QlUhWKkzjIIdB4+jqKQ8bDFCx46hvmDzARSMmohNe48BAI6XJpb2s2Opu3kzYFKJvVIvKinH/R8vxqHjvPIM455ef52Ga/41M2wxYsV/52wGAHyz2vkaxKzSvSP2Sv35GWvw7qxNeH56sImHmMxlweYDYYuQwldLd6Bg1ETsPlwc2DnjrmQnLdmBb1ZVzYXuYx+nLmeRi3sjZBg93vppIwBg+fZDyK8T/cXbo+BJCWMsISrE3lJnGMZ7gvZxR6EjyBRYqTMZz/6jJTzmYpOgdSzrdO9gpc5kPN0fnYLCR6eGLYZjKISEKMVlFfZ3CtHcXr3zcKBjDlGGlTpTJSgpd6CkIkaQKvPsx6c73tdRJ+SyQ+g/7ltXMmcSrNQZJgT2HS1BRYV/arrMZSd2sMi5uyosgz3MjnvNrsN4durq0M6vhJV6iDzz1coqG3ZVlVm7+wh6PDoF78zaaGu/4lLrk6LaP/ClXbFCJe4+9atenolxU1dFYuyGlXqIPDd9DUa8PjtsMWJNUUk5JizYGrYYtpBXuF+45aCl8rLle+d78/0SKXTiHv0ij0FEIR+8JaVORBuIaDERLSCiOdK2hkQ0hYhWS38b+CtqOnbDrsZ+uhSfLdzmkzRMGDzy2VLc9f4C/LxhX9iiWKZUchNUz7VmU5VLbhpHg5c6fDJ/K4a//JPr4yzZejByk7X2Hy1xvG9FhcAbP6xPSRWx54j5AGyU0hzYsdTPF0J0E0IUSt9HAZgmhOgAYJr0PVDs1uObP27IaGunKrLt4HEAwJFi+xkBqxIVFQJ/+mwZNu5NZEC8+4MFyTcGN1zyj++xdNsh18cRHjpgrn11luN9v1q2A498tiy5cEjBqIkofGwqvl+9B/uOluA/UkoEPSiMUCUVbtwvQwCMlz6PBzDUvTj2qFBo9fCrkgkTArB537HYuWL8QMtqXLXrMF7/YT1+/e95gckRln5btt16J7Nl/7GUUMgjxQkL/VBRaUo9LtxyAL99dx7u/XAR1kupgWes3JX8XS4ZBT1kVakLAF8R0VwiGiltayqEkBc23AGgqefSmeBj8AATE5QP3uUv/Ii73l8QojTRoFzjwSBJ3ZRXRDO0Myjvxeqdh/Hx/C3J732emIEz/lw5h0FuT0SUVo+y8i+R3GA3vfGzYj9I+6Web+7G/Rj2wg8oLgsu86fV3C99hBBbiagJgClEtEL5oxBCEJHmbZE6gZEA0KpVK1fCqqmIkB+L8ZZbx8/But1HMP3/zrNUnogs+T6jBPlk15WWayh16VRV3RDqP+5bAMDl3Vto/q5UzmYhkkTpnZH6+wMfL8aKHYexdtdRdDqxriOZ7WJJqQshtkp/dxHRxwB6AthJRM2EENuJqBmAXTr7vgLgFQAoLCz0tEmxTs9cpi7fafj7zHV7sfeI8wGxOGHX36yljLIkpR7kgJ6dU0XtWSYApWXaQmm5lfTuUZZUOEgD1NT9QkS1iKiO/BnAAABLAHwKYIRUbASACX4JqQdb6lWXq1+ZiTverfQPR8GXaRUvm+2Q53/Au7M2pWzTftUnz8+diSiVs53JTHK9qqs3Oyt4pW7FUm8K4GNpVDcHwLtCiElE9DOA/xDRLQA2Ahjun5jacPvMLPYfLcHm/cdwWov6ro4jhIhEFIJXGOmDhZsPYOHmA/jlmZWuzWPF6Uo9K+l+Ce6psXMLovIsy9WTRZQMPdWDUCm3nvxZWfJYRoQsdSHEOiFEV+nfqUKIP0vb9wohLhBCdBBCXCiECDxQmC314PnPnM3YIYURes0VL/2Iy/75gy/HjhvFZeVJi9tuK1eHd743e5PCDeBOroJRE/GPadGYDu8HFUqfuoN5AWr3VhidaaxnlIpoDuRnLPuPluDeDxfhxjf8mQW7brf9VeS1npW49vXd/vQV/jk9oTBPG/sVuj0yJfGDzes5plpjdfRHi5NK3Yt48L9Nsb+gtBlRmbwj1w8RUGanB9Rzv5BsqXsgnEVirdTZUg8WuZHbiTIpK69ILkjsJ3HwtizakligebkUR320pAxv/7QhqdAOHCvF018lFGZxWQWKbOR6UaKVzCsZ/WJBufjlKnh31iac/ugUX47tlq0HirB652FF9IvSuZKKpiGhN1AaRfdLlKnqSn3bgSIUjJroycxAO+w5UmI57vapySvR96kZ2HagyBdZtB6mqLaKSUt2AACmr0gEin00bysemrAU367eY7ifXetaq7SdTu9Xb82xdT6r3P/xYux1MYXfT85+fDr6j/u2Mk4d5m98WuM26n1kSz3IN5GYK3XFlxhYal7z09q9ABI+UzO+XrkLBaMmYsMe+y4OLZ74ciWARGOdsmynbhrZH9YmFJbfMeR+xXzLHD5emmJVO0GuoiyVMjju0CKfuGi75nYtEclGaJ3c6aQf19q1m3VCUXG1GGH3zU/vkrIkDVvOSt0acWgcQWCl/X0yPzF9fv7m/Z6cc9O+hEvlf/O24ldvzdFNI+u3stXCbrs4VlJmqljHfroMD01YmuxIHcml8NeqUU45T9tP53L09tFSqvIp/Xy7dXPkqDzKldP9KeV6zNqUSPuQIIuCd79YnVEaOrsPF+NIcRnaNK6V3FbVZ8clL9+C3lQ2Vi+QFdOOgwm3ynaTiBh7E1GsF/ZCGXQaMxn1a+ZiwZgBumX2H0u4DdSDkLaQ/bUaPymnnOvslr5d9cPy7Ycw6NnvcPt57XSPoX5m7CRCs1rXZm1MiPSOzcuEXm6Q3zjfnrkRa3cfMSyrdZXq6wgjTj02lnrvv07D+U9/nbItpaIEkgNQVYVK/5+5olbnppi6bCemrzCetZl2DEWDlc+ol/MiWc5CH3K0uCzFfeOks1aex8njc+CY8eIGyRmZDo4tI7dXtS/WrIqsdnIz1yXeIiYv3WFwrNTvnR+ebOnYVQVl9fyoeCtbtTNdwae0OZ17JPvUg0y5ExulrhVepFTqH8/fikHPfocpy+wpKrdMWrLdNB2nX9hRMOqyt741Bze/6XxATJ1LRO0nNju/zCX/+A6nPjwZf/p8WXKbHasmOAPIvcXltazqKld3tErkCCQ3LkuvxNc6TmTcLzpyfKqxDoMyx478HKj3T0a/sKVuTElZBdbvOZpSgbukDGrrTF6ZvObX/06k4wwCdZpQmFjJSvwaf9CzPmXkrf/V6fiWbE28Xf1vbmXmPLeiKvc3slrtUJk7xfkx5F2zbD516lMu2XoQBaMmYs2u1LZuNBj69swNmsfygzgPlBp12k7CZpOTjzik0ZiHPlmC85/+WjOiwkrFT1+xEx0f+hJHY7awQlqaUHngzcK+SZ+6i4BupZtH/lzpqzfmnVnmEToyXvkf9xwpxm1vz/XkWJRU6qmyWV1l51hJGd74YX3iWKrasntP5NW71CsOyQpEa1BOFtvdm0Y0B1mjhPo61M9JEMRmoFTJj+sSYXKHjjtTyk9PXoXjpQlrv3Pzel6KFihm/uzUwlJZj85Nqnd9XRkcdCJmukNevSflNMr9pYt1Ms1bj8oZmamMsDi79vEvV6RMQVdi7lNXlTcZwDCaZRukxaiHLMHjX1Zm8FZ3GE7z97zw9RpL5SYu2o5qOek27ZKt1taNtQp58IZnl1gqdaOE/2GE0IWFnYgWo3A6J6zYcRi/eOlHdJWSb5n51O1g9Pq+/WARzn3qa0tlvaRyDCH1fCt3HE5+nrNhHwoLGmrurxyItVtT6ivM0jmAUZ4RuZ68cB95xUvfrNU/l0aEjBWenLTSUjllhk8lnyxwt4bxwaJSNKxVLfk9qdQDtNVj6X6xM+W5KmDNpy6V9ajTW7/nKH7esB9zNyXi3vUUjROMjEndHOopkQjSJhOZKioEfmdxzdrka7QA1uw6gvE/bkh8V5S58iX9hZyVsmxQpU0wkzM9SZTe+EVi+85D+uGlfqqWPYeLsdXCzGHtafbG3+PCZf/8PuU7Gbw9+UUslLq6UctN2umIctwajBBCc1q+HfeLLVeN3jE0ak7235oNlJqiKBhU9MvRkjLNqAYt5MtbvfMwLv3H93j406WSAM7P7xStDvRIcZnh6kbe+NSNf39u+hqc/fh0xRbtu2/FalU/85e/8AOec5kd8qQHvnS1vxUOHy/DVEUEXtJL6fuZK4mJUk/9nhzlNzDpysor8OzU1ThYpB9/HNUkUOUVIiUp04vfrMXJD05KKycUDhgz7AyqGhwkDeWg3OZ9x3DouHG8t6XTOIlTd3BlVn22CzYfwOfSlPznpq9JJtoSQlh+rXZT71Z86lv3Fxm+LcmHcKLUhRCa4xNK15PfzN90AM+4zA5pZ9ELLawORzzwyeLk50pLnd0vhlix1D9duA3jpq4y7N39qOfS8goUuZl1CGDAuG/QXrIqth8s0vUTOlkuzJ2lno6s1P8xfTXOeXIGBkprQMo4OZ+tGaXSX60H1nxmo7Xz3POB9mLWQtiYZWlQEXbqaMg/v0eZht+xrKLC8HqTK/M4aPN/+nwZTnrwy7Soms8XOfM/a8ogDL9GAnUIqR47DxVjrxSZF4bdGAulnnaDDUK35Adk6/6Eby9PY4TbT655ZSZOGZNuVdthrSKv+NWvzNQtVxmmaH7Myppy3sy0HkbZ8jtemlA02zxYQMNWgIZU9k7FwFdRSblmx6r2n7tVHALWLV87tb5oS2qoovJtYOGWg9iokcq4vEKYnEQo/rfHOzMT4ailPg5ipfnUo6jVbTBmQsI9F0b0SyyUuhori7kek16Ra+XpB/iMmbDE07A3AJiz0ZuEWTKGOVUUaULNcGKpt7v/C1z/2qzKY2ioBLOFBJx0IbZ86pJMyvDWHo9NwSljJuFdVZIxtf9cfRq7r8gJ90sqUzVmNC/ddhClBvWktrDNVn+qlp3+2M5ev8+wrpOX5ib6JUDFpOfW2rzvGB77fFkkQjONKJb0ShCJ1NTEQqnrDpQa6GPZis8xcDTO23QA00xWrQ8bK0rRmqLW7wCOl5ZrKrTyCoHvFLm+tdqlH9nntOOsBcZNWYX1FlIHy/s/N90kZtmi6HrFKjTcL7e+NQcHjlVG6GzcexSDn/s+OWFIE9Pol9TvOdnpOzw2cbmhi8eNT91rH8KholJTpawn5h3vzsOr36/HshDzPAkIzLVovHH0i0UqQxr1a0pWNtkaSj3K05TVGCls+Sr+PdN8tmalpZ56wH1HS9DxoUl44Wv9mGEjzJS6kwkkWvdn56FiPDttNe5Uu1BcxV2n7mz3WHrW5KZ9x/DBz4l7ss+DRSHUcmlNmgGshZW6avkePTY9/zINz6rGuqw+k0GmsNXjvVmbcMWLP5qUSjWiOPpFhbpC5NdVo1d/+eZ7OSkGAOZu3Idl24KzEqwMfmlxrKQMBaMm4l/frkuUlbbP2ZC6SpIc0/yphUkXWqfz4zVYMyRP57Fw4+6ynkrW3v5XvTwT9/1vselq9GbHT55H9T1bL3zUyACQhHVi0HjtogSQlnjPrlTbDx4PzThba2ctXZ30En4SC6WuRm68WtEvsmWYdL9ovKq64YoXf8LFz33n6hhHi8ssr0DktE+SJ+m8KU+SkerqZUnJOzmPpotGY9vH8yuTczkRX0uBe905A+muiD9/sVxHHntUhjw6kcoc/TkB+nWUzCLo4rzq+2K3s1NSKy/b+Fw6x5Av/VdvzcEjny2z3HGGRRi5X2Kh1PVusGGcuvSbF1a1EMLTnnbE67Nxnio3vB7Gg1+pMm09UIRXv0tX2l8t3YEZK3fbkFDvfOnbtF6Hx3yyNPnZji42mjzjR2iY+jSvfb/e1v5m/mmvpoZbbXtGC3gkx0ldiGRltqhValZLDWBIG7S2UHdv/rgBf/psmWk5r7E2eSrxNysE/0sslLoaoyWikqPN0m/v/+w+13mb0V9gyPPGEQl2sOMysDL4JXPdq7Pw2MTlaZkDXzTIseHWR2ke/aIvvzrsUH3vUn7zwVL3MsWvH8e3y/0fL9b97eAxc9++erESNYOf+173N7uoxwWcjm/IC4NEFc79okPaa59B4iIZM2WjPpYZi7YcDNSXLmMpTA3A8zPWJCNDsmwkYlm6zXpWOs04dRedgjKjHkFbcR84VoLl2w/5MvvX7WxQs72tRpocdpht1A4Lt5jf51Mfnpw2gKnHwaJS3egi+arfm70Jp+rM2bAzjlAwaqJiP9ItFxR2OmtlzqCgsKzUiSibiOYT0efS9zZENIuI1hDRB0RUzewYXmM0+ciPuFC7vvQjxWXJnO3PTl2Nro985ak8yit8anLlrNMlWw8m1540U4Z2ptdrKUFTnW5wePViz1oxvcNe+BGDnv3On5l5LkMazdwiVvs7dURP+nmsHccLrObCmbtxn/6PCoGP6riE0tYodXiNQcZ/20G+vkpLPTjsWOp3AVCOJD0BYJwQoj2A/QBu8VIwJer7lnS/eGCp+9kmOj88GadKa0COm7rKMA+NHlYiGtRc++osjHzb2lJ1sqLedqAIK3YYv4lonU5ryrrVKk1bq1PDp75Oevvwxf3icn+zJuaVwlF3psdKwl/cxejSnFy144lg0dTpGtcT3LktKXUiagFgMIBXpe8EoB+AD6Ui4wEM9UNAbXkSf41e/YPuwQtGTdRdss0NTpXZ5n32BrUOHS/DwL8bv4lo1ahpnLqN3+S3Bq1758tAqcsmMlFK8uXX8fWO8585W7QLRgQvrlvvEOrHYd2eo5GIXdcjOfkowN7H6iIZfwdwL4A60vdGAA4IIWSTYQuA5lo7EtFIACMBoFWrVs4lVR5T+mt4Lw1+W20xMY9d3p650byQCb96y/li0Fps2V+ERrXzdH+3lxRMI6TRywfKIE+GH4+E2YP2xeLtmGow49hoYBKA49V74o4XkWK6IY0a22ZFeLA0krlfiOgSALuEEI4WexRCvCKEKBRCFObn5zs5hJZQAIxdLEYPrFIReTlI5cWNU0/KMHa/WDvmQtVall5iR6enLJoNpD2hyeivgJ4AM9l/8848fDRvq6vje6LgXB/Be9YZTMCxIq/pWI50kDJ1HLrGA1Hsw+QoI6zcUgHgxa/XYq1kQEbNp342gMuIaAOA95FwuzwLoD4RyZZ+CwDOW78Jafmkpb+aoW82j33v/xY5Cos67EHecCsYTbrx4pVun06om3J6+/CXfkJpeYXls5VXiGRYpVL8WetT6zl9AebEX+1FHrx/LPzuPN6ZuREvf6M92Svu6E3UAry1SttbWNgiaKVuhUNFpXhi0grMWi8NKEdpRqkQYrQQooUQogDA1QCmCyGuBTADwJVSsREAJvglpF5Io9FAqfKn92ZvMpzqPH/TAVRUCBw8Zl1RdxnrbSSLHkZjA160E61c7TsPHUePR6ckv8/esA9b9hdZPl9RaTm6PzolLV5eT4mrCcr/6Pdz9rcpqzBp6Q5Xx3jrpw2BTjH34lzzPXgztNMG3C5+4Qf7VcbSQxOWYvO+9JTJfuAmTv0+APcQ0RokfOyveSOSOXYnqYz+aDGen2GcsW/c1FXo+qevXCVgWmxhJXK7D02pRmd0rKQMa3Yd9k317TpUnLYtIbe9Mx4oKk1R5Gbu5WRKZY1n1KtrjXrKVjVjJiyNpPvFCMOMlDqonwu9x0TLlehHbhq3yCHFSqxG5LnFllIXQnwthLhE+rxOCNFTCNFeCPELIUS6JvCINPeLwYxSvX0OGMyoExD4YnEikkFescSaXPZvkt4ux0vLMfqjRWnbtfJw3zp+Di585ltfLE29ySKAe8tWPSdK+bW8Qhjmnt7kkZWjtCIjGuKcTlzkdIH6Eu1cstb6vX5i5S1C6+XBKA24l8RiRqmaLAP3i/4rvTGVoUfWcZKC4AOdsMfPFm7De7PTf9NKWPTj2oRv2mnY5hs/rNf9TW+yCGBftySiPyq/ExG2HyzC6p2Hpe+p5zXKPT3sBbNUp9ZlSn6uCtrSJlGpETsGU9CW+tKtVmaWp8uvlQbcD2Kh1NXVk4xn1rBiJ0gpZO0+sJWRF9b3WWRh6jUAfL1yV/Lz6I/Sw+DenrkRf/ww3Ur/aukO40keDpX6Iw6SIAl4YakTev91OvpL65jqRUBkik/dKzo2q2NeKMJotVO18aVe4cvOrQlaqR/WcK2o0WpbbKlbQMtSn7txPyYsSA/EWW6wUooQTlPcWmt6N77xs+HvD32yRHP7yLeNo0j9Gh/Se511omxTLHWD35Tfg3J7x0Sn49QT64Utgiuc3M8KISwPLEbRp651yWypK0hbzk52v+jcy7veX5DWU/68wdu1Q80IYir3s9NW+XJcvRSubi3bNCWu/l36O9TDjJhGWHVfqSMZMhk/3l606vnLJcZRQRUVsLzUZBSjX7SuOScrGHVrdUZpZLj7/fmW0gTY4anJK5EnpQL16tW/05jJnhzHCL8sWr1QUScPvNLF4sdCF26wej0HbIS6MumUVwjkGq+Jkb6PsP4kBpHl0i5auinb4wV79IiHpa74/MmCbUlFYRinbvMc8gQGe4orWkoqaqRVpYn/JYgp9cVlFejy8GS8O2tT5FfNkVliIVTWDnbmY3iBE2PAjsEmr+4VJbSkD8qnHjtLHVC6X/RvvHq6vVXsNcC4eGXdI4R7n7raUjfzsfvBjoPHcbi4DPd/vBgdT4jHAKSZq8IuZtk4vaZCCGyzuWpSlJN0WUFLj7BPXYG6guSQPj+C+TnMDbp9lV2LS92E0+LUTXzsfrNix+GAzxgNgm7h5ULgwme+sb1PnNGK+OHoFwv4MTvQTlua62Il+yjjV8eWliYgLW1AdNxZhwLK7RMGRm3cD5eUqDBeP1WLigoRSV+5VTTX2Q2ofcdDqes0Qj9e0V79bh0KRk3E54ukeHeDJ2DVTn9S+IaN9iUL76NfQrbUjbj9346SksaC3QazptXx4l7gZJLcwaJSPDPFn+iuICgqDXaWq5J4KHUd/HC/fCJNXvrtu4klxmLu2vMMJz71tEljZkvrBeFTP2RNaf2wJro5ut2WC8ECAAAgAElEQVTyO5Pl87zGiSvl71OtrZXKpBMLpa6nTIJY3SiqayAGjdMZpcpXTrW7Jd0q9F+rK9dyZYLByTM0x2gNVMaQeCh1nTbhh6WuHqE2cvHkBhR3GjSaKw8J9wNsyqpduu0g3pu9KeX3PTaSqTHxYf9R++MTx0vjEW4aRWKh1PUo18rR6hK1ojYyMqplx7r6dNG65AohHOWaUdZmlkKrb9gTTG5pJnzW78nMsaeoEgutpKdK/I5l3XagyNAfmJsTi+qzjZbyFsI8h40Zmflew5jBLq9gibVWmrnOe7+b0u971uPT8co3a3XLRm3au1foWeoHi+y9RqsTpel9ZjKbtQbrmTLeEwulHuRyXmp+WKsfBRH3WW92cHILft6wT5U3hTQ+MQzjJbFMExAkRiP3cVsazQ1OwtLUuePTVoZnGMZz4mGph3jurfv1c1bEfSqzHY57MJlC+WazYS8PlAZJ60Y1wxaBCYhYKPUgUcfE7zqsH2YX1EKyQaPVVxV7vBDBE5NWeHo8xhh2d1UdYqHUgzSI7cTHZqr7RWsMwwtLPTNrKx5EKa8O4y+xUOpRJVPdL1p9lRdLhmVodcUCVulVh1go9aimw81UJaU1OOxFuoTrXpvl+hgMwxgTC6XOBIuWAi8rz9AerKrApnqVwVSpE1F1IppNRAuJaCkRPSJtb0NEs4hoDRF9QETV/BJy1yHOCRI2nNgs3mTqRDkmHSuWejGAfkKIrgC6ARhIRL0APAFgnBCiPYD9AG7xS8j/++9Cvw7NaOCX+4UJD1bpVQdTpS4SyBl5cqV/AkA/AB9K28cDGOqLhAD2Hi3x69CMBlp50mb5kJIhSpxQt3rYIjCMJ1jyqRNRNhEtALALwBQAawEcEELI601tAdDcHxGBQzZzjjDu0LLKP5q/NQRJgqNWXnbYIvgKe1+qDpaUuhCiXAjRDUALAD0BdLR6AiIaSURziGjO7t27HQlZlXKsRIGq6Gl5Zni3sEXwFfUCJUzmYiv6RQhxAMAMAL0B1CciOXdMCwCappwQ4hUhRKEQojA/P9+RkOzPDZa/fLk8bBECp2XDzJ5Gz5Z61cFK9Es+EdWXPtcA0B/AciSU+5VSsREAJvglJBvqwZKaWbFqkEXAJac1C1sMhnGNFUu9GYAZRLQIwM8ApgghPgdwH4B7iGgNgEYAXvNPzODJy9AFMBhtsrIoo6fSZ/K1yXh1iT0LGnpzIAW92zby/Jh6mKbeFUIsAtBdY/s6JPzrGUnXlvUxe31mR3w4pVa1bBwtcZ8LJkpkkf9e5/sv7oi/fBFOIrPMV+lANhHKPHDV1qjm/aD5O7ee6fkx9WBzVIesqvAU+ECdvHim6M8i//3OYU4AqgKGumf160ddZQWoUFip68Az8PQxepXPr5sXoCTeEYSlHqYLpCo0Z6+uMe5VxUpdh2w21XUxqpncrHg2KSL7SnfJIxfhtnPbWj+HXaE8pCqENPIzmyCeT2AAVIWBJT/IyY5nvWVrWOpN6hi/ddS26WoKU+dEvTlf0aOF62Nke3SRR4rLzAtFGFbqOsRUNwWDQd1Ui2nUUBZR2nW1aFDD03PsCzFUNOrN+YberV0fwyu/9aqdR8wLRZh4PoEBwD51Z3hlLQVNQqeTapu311Ia5sLbEb8vXojnlfsl7l4cVuo6BDlaHTfkmrmn/0lpv8W1MySiNMXCTSBeeBf9Eu8bn9FK/cHBpzjeN2yLs11+rVDPb4Tc6M/QmqQR4+dBLbqlwcWYzHaO8W2xTLZH2izudZXRSv2cDs5yzQCA10EcfU+yJ8vTv+jqrQA+oPW668cDUdOHySBapPXjAT3dJzet4/s5Ym58WoIt9QQZrdSdMnpQR/zmvPaGZcbfbG8yrZVX+a4t6yc/+zGrzQmDu6TnQ5HbvJZlFOfnwYlS8MJQv141SNiqYU3Muv8CD45cSdRvixchl15do1dtOKy37YxW6k5vzinN6qJTs7rJ7xd0bJJWxq57xqz8uKu6omuLesnvBY2i4X6ZuHh72jb5SrSUYFx96kB6exEhZgf1uhbjbn1awau75VVNfXT72R4dyR4ZrdSdIpD6gNetkev6mGYDr+eelNpxVM/Nxqkn1tUpHQ1yNHxUfij14HRrquxBZQfVrDKPqzHzVbp37cSrNhzWnI2MU+qNa7tf/1oIkWLZeBJuZXIQLZ0fVeNKrhutcYeoymwFtexGefxvPrsNAHvWvF5R9Xbhw+hr1O+LF/J5VW9e1VVYM1xjodRzdCrn2z+en7atVl4O2jep7ep86odM6e+bfHdfvHHjGbaPadZQtF6Pozi1+8+Xd05+DqrRWnlYe7V1ny71v3M2p55X57QLxwzAmEs7uT6fEep737CWO2PFTVtq3cj/BUQ8UeoRs9RzvQrHsUkslPpjQztrbm+l0dgIldaT1VvTvH7qzEEjJXLyCXVwvoaP3S3ZWZRslLKujKp1JYul9fbhxnfrJgT1sq7ul8jVDNHUoF5N9+44JVpVpt7muilEtC3JeGHARG0xnbDmOcRCqdvRE06Uyjd/PA/f31dp9at7fCc3p3fbRnjumso09GZWhPIcYy87FYCzhTr+OqyL7X1khvUwV4xCKKNftAZKHZ8evzi9ZfJzn/aNk59Pb90grex9A1OXyb3wFPcd7dDuvq2dDkDfWNBqG2nRlS57eDd7+62bXrquh0eWerTcL2ENTsdDqRs0q44npMb4EgEvX1+I63u1Rrt8a26YnOwstGhQE3+TYsM7NksdoHRyb87vmI/Lup6Y/G7mQtB65XOyGPI1PVvp/qYVxaOkUzN7A7Oex6krdla+Pb18fSHObFNpRZ/ZpiFuP6+dhydOkOtgYMuOHnGjc7Ta4Fs398SbN1lzBeq14foW3jr8VE5nFDTAwM7NXLs8Tj2xrmcjEXGO4AJiotSNHthJd/fF+r9enFK0fZPaeHRoZ8OIkzkPXpi27YrTW2DD44PT3DF2Xw07NKmNG89qY2sfrYaUb5Il0C5mo/FWlI7SseV1SKNyV2UnWDsvB12aV4Z7+hVVoJbdyqXYUSQVOv4BK+fRKnJKs7o472Rrbyh6bbhP+8aYek9fS8eQubjLCbbKW8GtHiUCfmlg0AQpS9jEQ6mbkBqpYu2O5GQRqlkYyKidl5OM8rB6s287t11atkIr7he1Ne9149IKQVRiKXpACEP3ixuZlbuWlqfKouygtToOpdK6rpe1h1uOYJExqx+3uPH5atWrndS/6v0vOrWptJ3Qvom7Ga1/v8r+G6WMehzJKQTCHwak5yKqisRCqdu531bL5mZn4YdR/TD1nnN1y7x03en48q5zkkd9dEjlgK1d5SU/z3p+8pSOKVk2G38d1gXf3Zse5eMEr6NVtK1yN5Z65b5FqjVQlUc160xOa14/7XctHrokdWDWr2CFnCzC8MIW+N0FxrOUjVBb2i9dd7qtWccdT0h1rfWTXHFW7laaf1+1xZuxiNRjfn5nH/tHIEKtatlpnbUR91/c0bxQzIiHUrehQa0WrZWXg/w6eYbhjwM7n4CWDSsjbNz47MwsdT2Fe03PVmjZsCY++639Rq7GC/cLYKwI3PQbyl2LSvUXttaMulF87qExsKoVWaNuV9kqS92rLrB6bjaevLIr6tdMD0sc1NmaKyOLgDGXdMIJdas7kkF9X2y5FANwR6hvaWeFu80K8vUt/dNA03DTJ688LRmm2b9Tev2HOJHYE+Kh1KW/gzqfgGt6tjQpa9wClzxyEVY8OtDW+eVR9a37iyyV15JAPsaJ9Z0tvNDJg9mlevH+MlbaslkZK53qHee309yu3DfNUrfhYtNa3MJKvavrx4oxYUUBGE1iurNfB83t6nMTEW7u0wantbCn7GTycisf9aeuPE1x3MRfO9atH0re9eCkjf37tG+ccgkDT01V7F5O/rqtb1v8+5YzPTueFWKh1GWq52bjr8NOMyyjFbuupHZeDqrn2kuW1VOKvDhYVJL2W+Pa1gYzc7Oz8OzV3fDur5zdYD19XLd6jq6SlGkgRTioLVE1erqnd9tGyc+183KSz49W47fycOp1vMrtjww5NfU3xS7aicRIs6xMtews/OuGQkO51G9LpzTzJnuillKXo7as6qL0cvYUzx3nt8dt57bFikcH4heFLdMUs1F/r/WTF5O9zM7hhj9edLKlckKItJDZsnJ7dTvvof66v42++BT06dBY93c/iIVSt9LwF40dgBeu7YFnhnufsvby7s3x2NDOKbHRDaRX6cEWIwGu6tkSQ7o1R7N6+hajkdVHRHj9xsK0wbFFYy/CHy8y9gtO/N05yMkiXN/L2ZJhb9x0Br794/l4dGhnDO3W3PBtyFIkh04Z5fZT1GGlis9XnZH+tkYpn9NP0K9jE9TKM+7MlZb6U1ee5llSNbuv85px65T6V6vMmEs66Yat1qyWg9GDTkkzaOQrlgeiRw/qiB9H9VOdO70+XxuhHUr54a97a243w62lru6UfnNeO8w2yHRpZARsP3jc0bmr50ZDnUZDChMqG7L+01G3ei4u7tIMdap7O9svcX7Cdb1ap/hEO51YF+/9qhfu1/TVph/jfAuhZ7ISOUFH8ffr2BT//GV3zd+MOLF+Daz5y8WmLhy9187qudlo1agmru/VGllZhNycxAVqT5rxxmWRdlzFYft1bGq5rExWFplO21aOrxCRJUWjrLOz2zfSKWOwv86P6jPLssjtW2st2Jv7tLE921VWbvL5KoS1jrmWTuSN03QGrr0vacczv99uOFexPoLcIUZlRqvpVRNRSyKaQUTLiGgpEd0lbW9IRFOIaLX0N310yiNkRSHX2f9u740nrzR2w2gxc7S3Oap7t2uEvBzv8p7f0qcN3r6lp+HsyO6trFfz6zcWYvLd+jHIQ7qdqJkv3YzxN/XEb85rh2b10gftrDycSnfEqwqXiNG+ZgpW+bNeWbPonyZ1q2N4YWJV+7LyiqQFlp1F+OJ35xjuCwBN62gPYnox01GWfMylnXDfwI66RoJWLLx63oXyeOoNVvzJD1+iPxDpd1ioHfTaQa1qlR2S0zuj7FST54mLUgdQBuAPQohOAHoBuIOIOgEYBWCaEKIDgGnSd1+Rn43TWzfE8ELjAVMtTtBQQmGinuqelUU4p0O+6xl88ko6/To2xckn6PuFHxh8Cp6/tgdu7dMGL13XAyN6F2B4YQvT18i2+bVx78CO2knIpG3DujfXndSi1DtKd4ChW0f6qx7UAoB3bz0zuW+1nCxd/3CuBYUjP6wl5RVJC+yXPVvpvuUo9XWj2tVScuLLqPWsOpNo/07pbx5q/SArjrrVc3H7ee10J9ap78n4m3viB5U7JaW86vhCpN8H9ZmMJsVl25wYJl+n1vWMu6qrFFJsjubcBY3bPbhLM9SrmZvqrnPwvCkjsOTPfmTXdIJpKxdCbBdCzJM+HwawHEBzAEMAjJeKjQcw1C8hvZiEE2Q8qpm8st/TUTZJRbt591btQdfPf9cHKx8zj/CRG+ODl3TCwM7NUCsvB09e2RW185y7sORns+9J+bqTWpSNX/nZsN6kHztqDF6epcgTQ9B/SLVCOqf8vi8+/s1Zye/VshOdTElZBbq2SMS7y26VYT2a40QDw4CIkp3Uk1dUvkkqLfXpfzgXU36fOjeiSZ3qaeku0g9u/LPMA4NPwXW9WqWkVdBCfluS60S+b0IxuSx56rTv+sIoxyUWjR1gTWhoX97l3Vukja3o7q/lcnM4S3vwaeZvry0bVr79yLZCbNwvSoioAEB3ALMANBVCyMvi7ABg7Oh0gfxKp+VHNENuY5cq8rB4jfIBtsINZxUAgKZVZ4ZsfTSvXyNFmSnJzc6y5BbSd2k4b51yR2EUxqc8vN3YeN0HRzWQqIVWbpcOTeukuLTk8YLisgp0bVkfSx65CAM7Jx7yZ4Z3w48GLjyC4tIUp1KK3Da/Nhpo+J3fufXMlLTGdaonXARjL+2Ek5vWwWNDtDOVqmlcOw+PDe2SfFb0qqNEivCQ/c41pM7IrR9aOUvbSj+kflPQYsrv++IP/e3PFtV6memp6uwSbybq/cwlVwYnZFlp8wFieZ4xEdUG8D8AdwshDil7ayGEICLNKyKikQBGAkCrVs5yM1x0alPcfl473Na3re19ZSVgJSWAU4af0RK92zXC3R8swNyN+03Ln3tSPjY8Pjj5XX6ArVC3ei4euqQT+p/ivg/Ve4V/9uruuPbVWZaPU9i6AeZI133/4FOQm52FiyVf/d+v6oavV+7CJwu2JcsrG7/ys9GjVOm3NM6fYvRAKkM6775QOz48T2onJWUVAOxNxdfDyrPeqHYezmlfOfiWm52VbCM3msSQ3zvwZBw8VmpLprLyiuR5AODWc9qipKwCN55dgCPHy6TfCKXlwtZEJeUAqqU4/2TZym0Xqtp2h6Z10KFpHfy0bi9+XLtX8zhaMmq1heRkL8VP6mLqx+K2vm3x8rfrkt8XPjwgxcCUDZl6NXLx7b3n47SxX2nKGBSWWiwR5SKh0N8RQnwkbd5JRM2EENuJqBmAXVr7CiFeAfAKABQWFjrqynKys9L8z3bxIypGScuGNdGiQQ1LSl3JT6P7oWauPcVxSx/jh9wKha0boI6Owjq7fWO0za+FdbuPmh5n7oMXoka1bHQaMxlAwlJ8QjGIPbR7c9SpnpOi1JVKTtkgjJSA1bEoI6VeU5pWXz03C3dfqG35ye3ESYhdpxPrYv6mA2nbrcZMO8VskXQtypKWeuI6q+dm454BCTnzamdj3FVd0ahWHm54fbYt92fqAKL1/eSi1XKy8NJ1PazvqD6AWTGT2chA+r0vaJwa2lpPtbxlVhbh0aGd0ad9Y9Stnotb+7TBjJWa6jAQrES/EIDXACwXQjyj+OlTACOkzyMATPBePO9w4rqxi2wFdLGYewQAmtWr4fmiC1b48PazDLNYWlVqjWrnmbp61JZqRYr7xZqlLv9mtiSc0TFkq/t4aYVumRvOao07zm+HkRbfCmX5r+/VGkO6VeZAkeVo2bAG7jg/XenKbgBl6ls5l4upf92SXMa/l0iWeo7OG+zl3Vu4zhJqx8KXlW3Natm6MslYnaFprQ2LNEWv3q2HhYiz63u1RhtJ+T94SSdM+8N5lmT0Aysm4tkArgewmIgWSNvuB/A4gP8Q0S0ANgIY7o+I7hh/c098v3p3IOca2LkZ1v3lYtNFpv3gi9+dgwa1vOsc7FyBWVn1cmgpA6UK5UME3HhWAS7tmj5QJT9oun7L5Hu8vhw1LSTAysvJNp3MpUXb/FqSGNZeRh+6pBOu79U6JX1Bfp08vD+yl+28J1oM69Ec36/Zgw5NtQfjk5a6QVuVBz2tuKB6t22En9alukas6FQ5pNbNpDUAmL1+X9o29aXddHZB5bEMzqPsDPLr5BlGkEUR07slhPge+nXgbeC3D5x7Un7KRAG/CUOhA97khlFi55XbrGyHpnXw7q1n4peSnz7F/ZKi1Cm56lP6OYxPIitTI+uMiPCL01vgYgvRDV6hZ63mZmehQ9N0ZdGrrfYEJrsM69ECw3q00P29VOVT16J9k9oYPagjLu/eHHuPlmDQs9/pln3rlp7JcQgZs3bx0nWnV6YAlrZZy+lvreOU5yXUr5mLA8dKNe+FEMDlPZpj2opd+HbV7hRZ/m/ASRghBTXEiejMFGAihWydpa0wpIGVAbHGild5pcvF7iCLXnnZpSOLctGpTXFnv3S3x1O/6Gppdq9d5Bq4b2BHtGhQA6ee6N7a9pNSE/cLkLivt53bDk3qVjcNLczNzkqbZUogXHumfnBEjWrZybZTGSev3yLkKLiycoE/9D8Jjw7RNgCU8m94fDDukMYclM1U2WbrVs/FWzf3TH6XZWlUO8/3sTg/YKXOaCI37H42Ftm2Gimi9Kmf1U7fMp3y+7745o/nAaj0PddVPGRv39ITowYlXCXymIm8ePTL1xfiDwP8HaAE0juZwoKG+P6+fsk8M1GZkKKmVDVQaoX7BnZMJocDEoPiRsqeCPjz5V2w4fHBhvdZLmuGfI+Lyypw5wUdcH3vApyk417SPIfFcpXT/qN578xwH6/FGPLOrWeiaV1vl6WLIo8P65IWA6xE+UBVCIFFYwegWnaWYcZMpXvi6jNaoUIAVyuSeZ3TIR/ndEi41mrn5WDS3eegdUNvknDZJT03e2WYWxSx4n5Rc/t57VLe3LSWhFSidIX169gkJRyxWb3q6N6qMqBAnQpECzksWZYdAPq0z8eqnUcwepD+OEiP1onznKWRm0frfJXjNwbCRBhW6j5zts4Eoahjtz1fbbI+pFB9rmvztTbbQpZJ9eo+QGIAc/0e89BMr2nRoCbGXtopOXEpapRV2FfqdjGyjH9ST+JKOtX195EtdaVSLy5L5N03GgQ/vXVDLH3kotQYeo1yjw3tjMKCBpi2PBGO2MhhcrKwYaXOGOLHsG+Qb7VTf3+urw4Qo2sxmzQUJiVl9t0vdvFysB2olFWp1OWxC61BZyVqf7/cCSjjGq6TjIb2+bXRtnEtDLS4KlXUYKXOBI4XWQutElQ0khf5iYLEifvFLk4WhDdqGeef3AT/mbMlxY9/Tc+WOLNtQ7TLt5dH6cXrTsdH87Zo7peTnYVBDrKXRgVW6kzgqAegLut6IlbuOBySNO6I6kCoGUG4X5SYqfTKkEb9+hzUpRkWjx2QEpFCRLYVOpBYY+C3OksJxh1W6kzgqFeff+4a+wt/RI2YGeqOol/8RLbkzbrIOIYYBg0rdUYTv1wk7ZvUxlnt4jl4nEk8OPgUCJFIkRwFotG1ZAas1BlD4uYrZqzRulEtvDrCeCFuL7HajqIeGt6qofHC9lGAJx9VMS7zMa+8FTKtj4i6EooKZve9Rm42erdthBeudZChMUC+vff8sEUwhS31Ksa4q7o5Wt+V0aZyUYxM66685bSWiQlAN5/dRjNhW1YW4b2RvYIWKyNhpV7FyM4iZGeZZytkA5Txkh6tGqRFrsQNPxfa8RJW6owJbIEy3hBnhf7T6H7JJf+iTjy6HiZw5Cn3Uc1dEjW468tsmtWrgfo145E2gC11RpOxl3XCsB7N0b6J/YkdVQkeKE1Hzl/OhAMrdUaTvJzsZBpbL2ifXxsjerfGDTFcdMAIOQuBkzVNM5VJd/XFxr3BJ1FjErBSZwIhK4vwyJDOYYvhOfKi0sN6NDcpWXU4oV51nFCvethiVFlYqTOMC+rXrIY/X94lbDGYEHhmeFc0q1fDvGDAsFJnGIZxgNEasGHC0S8MwzAZBCt1hmGYDIKVOsMwTAbBSp1hGCaDMFXqRPQ6Ee0ioiWKbQ2JaAoRrZb+NvBXTIZhGMYKViz1NwEMVG0bBWCaEKIDgGnSd4ZhGCZkTJW6EOJbAPtUm4cAGC99Hg9gqMdyMQzDMA5w6lNvKoTYLn3eAaCpR/IwDMMwLnA9+UgIIYhIN60REY0EMFL6eoSIVjo8VWMAexzuGxZxkzlu8gLxkzlu8gLxkzlu8gLmMre2eiCnSn0nETUTQmwnomYAdukVFEK8AuAVh+dJQkRzhBDBLaroAXGTOW7yAvGTOW7yAvGTOW7yAt7K7NT98imAEdLnEQAmeCEMwzAM4w4rIY3vAfgJwMlEtIWIbgHwOID+RLQawIXSd4ZhGCZkTN0vQohrdH66wGNZzHDtwgmBuMkcN3mB+MkcN3mB+MkcN3kBD2UmwUu3MAzDZAycJoBhGCaDiIVSJ6KBRLSSiNYQUSRmrxJRSyKaQUTLiGgpEd0lbR9LRFuJaIH072LFPqOla1hJRBeFIPMGIlosyTVH2qaZ8oESPCfJu4iIeoQg78mKelxARIeI6O6o1bGdVBpG9UpEI6Tyq4lohNa5fJT3KSJaIcn0MRHVl7YXEFGRoq5fUuxzutSe1kjX5Nuafjoy224HQekSHXk/UMi6gYgWSNu9rWMhRKT/AcgGsBZAWwDVACwE0CkCcjUD0EP6XAfAKgCdAIwF8H8a5TtJsucBaCNdU3bAMm8A0Fi17UkAo6TPowA8IX2+GMCXAAhALwCzItAOdiARrxupOgbQF0APAEuc1iuAhgDWSX8bSJ8bBCjvAAA50ucnFPIWKMupjjNbugaSrmlQwHVsqx0EqUu05FX9/jcAY/yo4zhY6j0BrBFCrBNClAB4H4k0BaEihNguhJgnfT4MYDkAo4UqhwB4XwhRLIRYD2ANEtcWNnopH4YAeEskmAmgPiXmJITFBQDWCiE2GpQJpY6FvVQaevV6EYApQoh9Qoj9AKYgPeeSb/IKIb4SQpRJX2cCMFzWR5K5rhBipkhon7fgY7oQnTrWQ68dBKZLjOSVrO3hAN4zOobTOo6DUm8OYLPi+xYYK8/AIaICAN0BzJI2/VZ6jX2dKjNYRuE6BICviGguJWb6AvopH6Igr5KrkfoQRLWOZezWa5RkvxkJq1CmDRHNJ6JviOgcaVtzJGSUCUteO+0gKnV8DoCdQojVim2e1XEclHqkIaLaAP4H4G4hxCEALwJoB6AbgO1IvGZFhT5CiB4ABgG4g4j6Kn+UrIHIhUMRUTUAlwH4r7QpynWcRlTrVQsiegBAGYB3pE3bAbQSQnQHcA+Ad4mobljyqYhVO1BwDVINFE/rOA5KfSuAlorvLaRtoUNEuUgo9HeEEB8BgBBipxCiXAhRAeBfqHz9D/06hBBbpb+7AHwsybZTdqtQasqH0OVVMAjAPCHETiDadazAbr2GLjsR3QjgEgDXSh0RJBfGXunzXCR80idJsildNGG0Z7vtIAp1nANgGIAP5G1e13EclPrPADoQURvJYrsaiTQFoSL5xV4DsFwI8Yxiu9LvfDkAefT7UwBXE1EeEbUB0AGJQZCg5K1FRHXkz0gMjC2BfsqHTwHcIEVr9AJwUOFOCJoUyyaqdazCbr1OBjCAiBpIboQB0rZAIKKBAO4FcJkQ4phiez4RZUuf2yJRp+skmQ8RUS/pWbgBAez6WPAAAAEGSURBVKcLcdAOoqBLLgSwQgiRdKt4Xsd+jPx6/Q+JiIFVSPRgD4QtjyRTHyReqRcBWCD9uxjA2wAWS9s/BdBMsc8D0jWshI+RAjrytkVitH8hgKVyPQJohMRCJ6sBTAXQUNpOAJ6X5F0MoDCkeq4FYC+AeoptkapjJDqc7QBKkfB73uKkXpHwZa+R/t0UsLxrkPA3y235JansFVJ7WQBgHoBLFccpREKRrgXwT0iTGQOU2XY7CEqXaMkrbX8TwK9VZT2tY55RyjAMk0HEwf3CMAzDWISVOsMwTAbBSp1hGCaDYKXOMAyTQbBSZxiGySBYqTMMw2QQrNQZhmEyCFbqDMMwGcT/A2PoJWY/SavJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot output losses\n",
    "pl.plot(output_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Eval sub-routine\\n\\nmodel = PolicyEstimator(weight_matrix, input_emb_dim, lstm_units, len(X_vocab), len(Y_vocab), max_len, batch_size)\\noptimizer = optim.Adam(model.parameters(),lr=0.01)\\n\\ncheckpoint = torch.load(MODEL_PATH)\\n\\nmodel.load_state_dict(checkpoint['model_state_dict'])\\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\\n\\nmodel.eval()\\nvalidate()\\n\\n\""
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Eval sub-routine\n",
    "\n",
    "model = PolicyEstimator(weight_matrix, input_emb_dim, lstm_units, len(X_vocab), len(Y_vocab), max_len, batch_size)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "checkpoint = torch.load(MODEL_PATH)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "validate()\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
